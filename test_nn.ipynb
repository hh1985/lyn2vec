{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "# Define the length of the DNA reads\n",
    "read_length = 150\n",
    "\n",
    "# Define the number of reads in the dataset\n",
    "num_reads = 1000000\n",
    "\n",
    "# Generate random DNA sequences for dataset1 and dataset2\n",
    "def generate_reads(num_reads, read_length):\n",
    "    bases = ['A', 'C', 'G', 'T']\n",
    "    reads = []\n",
    "    for i in range(num_reads):\n",
    "        read = ''.join([random.choice(bases) for _ in range(read_length)])\n",
    "        reads.append(read)\n",
    "    return reads\n",
    "\n",
    "dataset1_reads = generate_reads(num_reads, read_length)\n",
    "dataset2_reads = generate_reads(num_reads, read_length)\n",
    "\n",
    "# Convert DNA sequences to numerical representation\n",
    "def encode_reads(reads):\n",
    "    encoding = {'A': [1,0,0,0], 'C': [0,1,0,0], 'G': [0,0,1,0], 'T': [0,0,0,1]}\n",
    "    encoded_reads = []\n",
    "    for read in reads:\n",
    "        encoded_read = [encoding[base] for base in read]\n",
    "        encoded_reads.append(np.concatenate(encoded_read))\n",
    "    return np.array(encoded_reads)\n",
    "\n",
    "dataset1_encoded_reads = encode_reads(dataset1_reads)\n",
    "dataset2_encoded_reads = encode_reads(dataset2_reads)\n",
    "\n",
    "# Generate labels for pairs of reads based on overlap length\n",
    "def generate_labels(reads1, reads2):\n",
    "    num_reads = len(reads1)\n",
    "    labels = np.zeros(num_reads)\n",
    "    for i in range(num_reads):\n",
    "        overlap_length = 0\n",
    "        for j in range(read_length):\n",
    "            if reads1[i][-j:] == reads2[i][:j]:\n",
    "                overlap_length = j\n",
    "        labels[i] = math.exp(overlap_length / read_length)\n",
    "    return labels\n",
    "\n",
    "split_size = int(num_reads * 0.8)\n",
    "train_similarities = generate_labels(dataset1_reads[:split_size], dataset2_reads[:split_size])\n",
    "val_similarities = generate_labels(dataset1_reads[split_size:], dataset2_reads[split_size:])\n",
    "\n",
    "\n",
    "# Split dataset1 and dataset2 into training and validation sets\n",
    "train_size = int(0.8 * num_reads)\n",
    "indices = np.random.permutation(num_reads)\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:]\n",
    "\n",
    "train_dataset1_reads = dataset1_encoded_reads[train_indices]\n",
    "val_dataset1_reads = dataset1_encoded_reads[val_indices]\n",
    "train_dataset2_reads = dataset2_encoded_reads[train_indices]\n",
    "val_dataset2_reads = dataset2_encoded_reads[val_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TGGGGAGAGGGGTTTTCATGTACGTGGGATGTTTAACTTGCGGAATGATTTACGCCCGGTTTAGGAGGTTAGACTATACGCGTAGCCCAAGTATACAGCCCAGCATTAGCAAGCTACGGTCCATATACCAACTCGGACCAGGTTTACCCG\n",
      "GGTAGGCAATGCGCCCATGGTGCCAATCTCCGCTACTGGGCTTCATGCGATAGGGAAATGAAACCTCAGGACCAAGAGTATTGCTTGTCTGATTGGTCTACAACGTGGATGTCCTATCGGGGTCGCCTTAGTACTTCAGGGGGAGGAAGC\n",
      "1.0066889383540194\n"
     ]
    }
   ],
   "source": [
    "print(dataset1_reads[2]) \n",
    "print(dataset2_reads[2])\n",
    "print(train_similarities[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.71600686])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read1 = \"AACTGGTACGTCAGTTACGCTAGGTTCAAGTGGCTAGCTACGTCGATAGCTGTCAGGCTAGCTAGGAGCTAGCTGACTGATCGATCGATCGATCGGATCGATCTACGATCGATCGATCGACGTCAGCTGACTGATCGATCGATCGATCGGATCGATC\"\n",
    "read2 = \"CTGATCGATCGATCGATCGGATCGATCTACGATCGATCGATCGACGTCAGCTGACTGATCGATCGATCGATCGGATCGATCTGACGTACGATCGATCGATCGATCGGATCGATCAGCTAGCTAGCTAAGGCTAGCTAGCTAGGAGCTAGCTGACTGATC\"\n",
    "\n",
    "generate_labels([read1], [read2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the size of the hidden layer\n",
    "hidden_size = 64\n",
    "\n",
    "# Define the input shape for the DNA reads\n",
    "input_shape = (4, read_length)\n",
    "\n",
    "# Define the siamese autoencoder model\n",
    "class SiameseAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseAutoencoder, self).__init__()\n",
    "        \n",
    "        # Define the encoder layers for dataset1 and dataset2\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_shape[0]*input_shape[1], hidden_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Define the decoder layers for dataset1 and dataset2\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_size, input_shape[0]*input_shape[1]),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # Compute the embeddings for dataset1 and dataset2\n",
    "        embedding1 = self.encoder(input1.view(-1, input_shape[0]*input_shape[1]))\n",
    "        embedding2 = self.encoder(input2.view(-1, input_shape[0]*input_shape[1]))\n",
    "        \n",
    "        # Compute the distance between the embeddings\n",
    "        distance = torch.norm(embedding1 - embedding2, p=2, dim=1, keepdim=True)\n",
    "        \n",
    "        # Reconstruct the DNA reads from the embeddings\n",
    "        reconstruction1 = self.decoder(embedding1)\n",
    "        reconstruction2 = self.decoder(embedding2)\n",
    "        \n",
    "        # Concatenate the reconstructions and distance\n",
    "        output = torch.cat((reconstruction1, reconstruction2, distance), dim=1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Define the siamese autoencoder model\n",
    "model = SiameseAutoencoder()\n",
    "\n",
    "# Define the loss function\n",
    "def contrastive_loss(output, target):\n",
    "    margin = 1.0\n",
    "    distances = output[:, -1].unsqueeze(1)\n",
    "    reconstructions1 = output[:, :input_shape[0]*input_shape[1]].view(-1, input_shape[0], input_shape[1])\n",
    "    reconstructions2 = output[:, input_shape[0]*input_shape[1]:-1].view(-1, input_shape[0], input_shape[1])\n",
    "    distance_similarity = torch.exp(-distances)\n",
    "    loss = 0.5 * ((1-target) * F.mse_loss(reconstructions1, reconstructions2) + target * torch.pow(distances - margin, 2) * distance_similarity)\n",
    "    return loss.mean()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "class DNAReadsDataset(data.Dataset):\n",
    "    def __init__(self, reads1, reads2, similarities):\n",
    "        self.reads1 = reads1\n",
    "        self.reads2 = reads2\n",
    "        self.similarities = similarities\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        input1 = torch.Tensor(one_hot_encode(self.reads1[index]))\n",
    "        input2 = torch.Tensor(one_hot_encode(self.reads2[index]))\n",
    "        target = torch.Tensor([self.similarities[index]])\n",
    "        return input1, input2, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.reads1)\n",
    "    \n",
    "def one_hot_encode(read):\n",
    "    encoding = np.zeros((4, len(read)))\n",
    "    for i, base in enumerate(read):\n",
    "        if base == 'A':\n",
    "            encoding[0, i] = 1\n",
    "        elif base == 'C':\n",
    "            encoding[1, i] = 1\n",
    "        elif base == 'G':\n",
    "            encoding[2, i] = 1\n",
    "        elif base == 'T':\n",
    "            encoding[3, i] = 1\n",
    "    return encoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20000], Train Loss: 0.0068, Val Loss: 0.0069\n",
      "Epoch [2/20000], Train Loss: 0.0066, Val Loss: 0.0065\n",
      "Epoch [3/20000], Train Loss: 0.0066, Val Loss: 0.0067\n",
      "Epoch [4/20000], Train Loss: 0.0066, Val Loss: 0.0065\n",
      "Epoch [5/20000], Train Loss: 0.0066, Val Loss: 0.0064\n",
      "Epoch [6/20000], Train Loss: 0.0066, Val Loss: 0.0065\n",
      "Epoch [7/20000], Train Loss: 0.0066, Val Loss: 0.0070\n",
      "Epoch [8/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [9/20000], Train Loss: 0.0065, Val Loss: 0.0062\n",
      "Epoch [10/20000], Train Loss: 0.0066, Val Loss: 0.0065\n",
      "Epoch [11/20000], Train Loss: 0.0066, Val Loss: 0.0066\n",
      "Epoch [12/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [13/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [14/20000], Train Loss: 0.0065, Val Loss: 0.0065\n",
      "Epoch [15/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [16/20000], Train Loss: 0.0065, Val Loss: 0.0061\n",
      "Epoch [17/20000], Train Loss: 0.0065, Val Loss: 0.0065\n",
      "Epoch [18/20000], Train Loss: 0.0065, Val Loss: 0.0069\n",
      "Epoch [19/20000], Train Loss: 0.0065, Val Loss: 0.0068\n",
      "Epoch [20/20000], Train Loss: 0.0066, Val Loss: 0.0067\n",
      "Epoch [21/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [22/20000], Train Loss: 0.0066, Val Loss: 0.0062\n",
      "Epoch [23/20000], Train Loss: 0.0065, Val Loss: 0.0062\n",
      "Epoch [24/20000], Train Loss: 0.0065, Val Loss: 0.0065\n",
      "Epoch [25/20000], Train Loss: 0.0065, Val Loss: 0.0066\n",
      "Epoch [26/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [27/20000], Train Loss: 0.0065, Val Loss: 0.0069\n",
      "Epoch [28/20000], Train Loss: 0.0065, Val Loss: 0.0062\n",
      "Epoch [29/20000], Train Loss: 0.0065, Val Loss: 0.0062\n",
      "Epoch [30/20000], Train Loss: 0.0065, Val Loss: 0.0065\n",
      "Epoch [31/20000], Train Loss: 0.0065, Val Loss: 0.0068\n",
      "Epoch [32/20000], Train Loss: 0.0065, Val Loss: 0.0065\n",
      "Epoch [33/20000], Train Loss: 0.0065, Val Loss: 0.0066\n",
      "Epoch [34/20000], Train Loss: 0.0065, Val Loss: 0.0062\n",
      "Epoch [35/20000], Train Loss: 0.0065, Val Loss: 0.0070\n",
      "Epoch [36/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [37/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [38/20000], Train Loss: 0.0066, Val Loss: 0.0067\n",
      "Epoch [39/20000], Train Loss: 0.0065, Val Loss: 0.0062\n",
      "Epoch [40/20000], Train Loss: 0.0065, Val Loss: 0.0061\n",
      "Epoch [41/20000], Train Loss: 0.0065, Val Loss: 0.0065\n",
      "Epoch [42/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [43/20000], Train Loss: 0.0065, Val Loss: 0.0065\n",
      "Epoch [44/20000], Train Loss: 0.0066, Val Loss: 0.0063\n",
      "Epoch [45/20000], Train Loss: 0.0066, Val Loss: 0.0066\n",
      "Epoch [46/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [47/20000], Train Loss: 0.0066, Val Loss: 0.0066\n",
      "Epoch [48/20000], Train Loss: 0.0065, Val Loss: 0.0062\n",
      "Epoch [49/20000], Train Loss: 0.0065, Val Loss: 0.0066\n",
      "Epoch [50/20000], Train Loss: 0.0066, Val Loss: 0.0065\n",
      "Epoch [51/20000], Train Loss: 0.0065, Val Loss: 0.0062\n",
      "Epoch [52/20000], Train Loss: 0.0066, Val Loss: 0.0071\n",
      "Epoch [53/20000], Train Loss: 0.0066, Val Loss: 0.0067\n",
      "Epoch [54/20000], Train Loss: 0.0066, Val Loss: 0.0065\n",
      "Epoch [55/20000], Train Loss: 0.0066, Val Loss: 0.0062\n",
      "Epoch [56/20000], Train Loss: 0.0066, Val Loss: 0.0065\n",
      "Epoch [57/20000], Train Loss: 0.0066, Val Loss: 0.0065\n",
      "Epoch [58/20000], Train Loss: 0.0066, Val Loss: 0.0077\n",
      "Epoch [59/20000], Train Loss: 0.0065, Val Loss: 0.0067\n",
      "Epoch [60/20000], Train Loss: 0.0066, Val Loss: 0.0066\n",
      "Epoch [61/20000], Train Loss: 0.0066, Val Loss: 0.0071\n",
      "Epoch [62/20000], Train Loss: 0.0066, Val Loss: 0.0063\n",
      "Epoch [63/20000], Train Loss: 0.0066, Val Loss: 0.0063\n",
      "Epoch [64/20000], Train Loss: 0.0066, Val Loss: 0.0062\n",
      "Epoch [65/20000], Train Loss: 0.0065, Val Loss: 0.0075\n",
      "Epoch [66/20000], Train Loss: 0.0065, Val Loss: 0.0062\n",
      "Epoch [67/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [68/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [69/20000], Train Loss: 0.0066, Val Loss: 0.0065\n",
      "Epoch [70/20000], Train Loss: 0.0066, Val Loss: 0.0064\n",
      "Epoch [71/20000], Train Loss: 0.0066, Val Loss: 0.0063\n",
      "Epoch [72/20000], Train Loss: 0.0065, Val Loss: 0.0072\n",
      "Epoch [73/20000], Train Loss: 0.0066, Val Loss: 0.0073\n",
      "Epoch [74/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [75/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [76/20000], Train Loss: 0.0065, Val Loss: 0.0066\n",
      "Epoch [77/20000], Train Loss: 0.0065, Val Loss: 0.0066\n",
      "Epoch [78/20000], Train Loss: 0.0066, Val Loss: 0.0072\n",
      "Epoch [79/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [80/20000], Train Loss: 0.0066, Val Loss: 0.0079\n",
      "Epoch [81/20000], Train Loss: 0.0065, Val Loss: 0.0062\n",
      "Epoch [82/20000], Train Loss: 0.0066, Val Loss: 0.0064\n",
      "Epoch [83/20000], Train Loss: 0.0065, Val Loss: 0.0067\n",
      "Epoch [84/20000], Train Loss: 0.0066, Val Loss: 0.0063\n",
      "Epoch [85/20000], Train Loss: 0.0065, Val Loss: 0.0067\n",
      "Epoch [86/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [87/20000], Train Loss: 0.0065, Val Loss: 0.0065\n",
      "Epoch [88/20000], Train Loss: 0.0066, Val Loss: 0.0061\n",
      "Epoch [89/20000], Train Loss: 0.0065, Val Loss: 0.0067\n",
      "Epoch [90/20000], Train Loss: 0.0065, Val Loss: 0.0067\n",
      "Epoch [91/20000], Train Loss: 0.0066, Val Loss: 0.0068\n",
      "Epoch [92/20000], Train Loss: 0.0066, Val Loss: 0.0065\n",
      "Epoch [93/20000], Train Loss: 0.0065, Val Loss: 0.0067\n",
      "Epoch [94/20000], Train Loss: 0.0065, Val Loss: 0.0065\n",
      "Epoch [95/20000], Train Loss: 0.0065, Val Loss: 0.0070\n",
      "Epoch [96/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [97/20000], Train Loss: 0.0065, Val Loss: 0.0065\n",
      "Epoch [98/20000], Train Loss: 0.0065, Val Loss: 0.0078\n",
      "Epoch [99/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [100/20000], Train Loss: 0.0065, Val Loss: 0.0061\n",
      "Epoch [101/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [102/20000], Train Loss: 0.0065, Val Loss: 0.0077\n",
      "Epoch [103/20000], Train Loss: 0.0066, Val Loss: 0.0066\n",
      "Epoch [104/20000], Train Loss: 0.0065, Val Loss: 0.0060\n",
      "Epoch [105/20000], Train Loss: 0.0065, Val Loss: 0.0066\n",
      "Epoch [106/20000], Train Loss: 0.0065, Val Loss: 0.0062\n",
      "Epoch [107/20000], Train Loss: 0.0065, Val Loss: 0.0067\n",
      "Epoch [108/20000], Train Loss: 0.0065, Val Loss: 0.0066\n",
      "Epoch [109/20000], Train Loss: 0.0066, Val Loss: 0.0066\n",
      "Epoch [110/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [111/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [112/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [113/20000], Train Loss: 0.0065, Val Loss: 0.0069\n",
      "Epoch [114/20000], Train Loss: 0.0065, Val Loss: 0.0067\n",
      "Epoch [115/20000], Train Loss: 0.0066, Val Loss: 0.0065\n",
      "Epoch [116/20000], Train Loss: 0.0065, Val Loss: 0.0061\n",
      "Epoch [117/20000], Train Loss: 0.0065, Val Loss: 0.0062\n",
      "Epoch [118/20000], Train Loss: 0.0065, Val Loss: 0.0066\n",
      "Epoch [119/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [120/20000], Train Loss: 0.0065, Val Loss: 0.0077\n",
      "Epoch [121/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [122/20000], Train Loss: 0.0065, Val Loss: 0.0065\n",
      "Epoch [123/20000], Train Loss: 0.0065, Val Loss: 0.0067\n",
      "Epoch [124/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [125/20000], Train Loss: 0.0066, Val Loss: 0.0064\n",
      "Epoch [126/20000], Train Loss: 0.0065, Val Loss: 0.0066\n",
      "Epoch [127/20000], Train Loss: 0.0065, Val Loss: 0.0067\n",
      "Epoch [128/20000], Train Loss: 0.0066, Val Loss: 0.0064\n",
      "Epoch [129/20000], Train Loss: 0.0065, Val Loss: 0.0068\n",
      "Epoch [130/20000], Train Loss: 0.0065, Val Loss: 0.0066\n",
      "Epoch [131/20000], Train Loss: 0.0066, Val Loss: 0.0067\n",
      "Epoch [132/20000], Train Loss: 0.0065, Val Loss: 0.0062\n",
      "Epoch [133/20000], Train Loss: 0.0065, Val Loss: 0.0062\n",
      "Epoch [134/20000], Train Loss: 0.0066, Val Loss: 0.0067\n",
      "Epoch [135/20000], Train Loss: 0.0066, Val Loss: 0.0065\n",
      "Epoch [136/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [137/20000], Train Loss: 0.0065, Val Loss: 0.0082\n",
      "Epoch [138/20000], Train Loss: 0.0065, Val Loss: 0.0067\n",
      "Epoch [139/20000], Train Loss: 0.0065, Val Loss: 0.0061\n",
      "Epoch [140/20000], Train Loss: 0.0065, Val Loss: 0.0066\n",
      "Epoch [141/20000], Train Loss: 0.0066, Val Loss: 0.0066\n",
      "Epoch [142/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [143/20000], Train Loss: 0.0065, Val Loss: 0.0066\n",
      "Epoch [144/20000], Train Loss: 0.0066, Val Loss: 0.0063\n",
      "Epoch [145/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [146/20000], Train Loss: 0.0065, Val Loss: 0.0066\n",
      "Epoch [147/20000], Train Loss: 0.0065, Val Loss: 0.0073\n",
      "Epoch [148/20000], Train Loss: 0.0065, Val Loss: 0.0066\n",
      "Epoch [149/20000], Train Loss: 0.0065, Val Loss: 0.0065\n",
      "Epoch [150/20000], Train Loss: 0.0065, Val Loss: 0.0062\n",
      "Epoch [151/20000], Train Loss: 0.0065, Val Loss: 0.0067\n",
      "Epoch [152/20000], Train Loss: 0.0065, Val Loss: 0.0078\n",
      "Epoch [153/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [154/20000], Train Loss: 0.0065, Val Loss: 0.0067\n",
      "Epoch [155/20000], Train Loss: 0.0065, Val Loss: 0.0069\n",
      "Epoch [156/20000], Train Loss: 0.0066, Val Loss: 0.0063\n",
      "Epoch [157/20000], Train Loss: 0.0065, Val Loss: 0.0071\n",
      "Epoch [158/20000], Train Loss: 0.0066, Val Loss: 0.0065\n",
      "Epoch [159/20000], Train Loss: 0.0066, Val Loss: 0.0063\n",
      "Epoch [160/20000], Train Loss: 0.0066, Val Loss: 0.0076\n",
      "Epoch [161/20000], Train Loss: 0.0066, Val Loss: 0.0069\n",
      "Epoch [162/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [163/20000], Train Loss: 0.0065, Val Loss: 0.0069\n",
      "Epoch [164/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [165/20000], Train Loss: 0.0065, Val Loss: 0.0061\n",
      "Epoch [166/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [167/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [168/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [169/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [170/20000], Train Loss: 0.0066, Val Loss: 0.0066\n",
      "Epoch [171/20000], Train Loss: 0.0065, Val Loss: 0.0062\n",
      "Epoch [172/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [173/20000], Train Loss: 0.0065, Val Loss: 0.0062\n",
      "Epoch [174/20000], Train Loss: 0.0065, Val Loss: 0.0068\n",
      "Epoch [175/20000], Train Loss: 0.0066, Val Loss: 0.0064\n",
      "Epoch [176/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [177/20000], Train Loss: 0.0065, Val Loss: 0.0065\n",
      "Epoch [178/20000], Train Loss: 0.0066, Val Loss: 0.0063\n",
      "Epoch [179/20000], Train Loss: 0.0065, Val Loss: 0.0071\n",
      "Epoch [180/20000], Train Loss: 0.0065, Val Loss: 0.0067\n",
      "Epoch [181/20000], Train Loss: 0.0065, Val Loss: 0.0067\n",
      "Epoch [182/20000], Train Loss: 0.0065, Val Loss: 0.0065\n",
      "Epoch [183/20000], Train Loss: 0.0065, Val Loss: 0.0068\n",
      "Epoch [184/20000], Train Loss: 0.0065, Val Loss: 0.0061\n",
      "Epoch [185/20000], Train Loss: 0.0066, Val Loss: 0.0067\n",
      "Epoch [186/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [187/20000], Train Loss: 0.0065, Val Loss: 0.0061\n",
      "Epoch [188/20000], Train Loss: 0.0065, Val Loss: 0.0066\n",
      "Epoch [189/20000], Train Loss: 0.0066, Val Loss: 0.0067\n",
      "Epoch [190/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [191/20000], Train Loss: 0.0065, Val Loss: 0.0068\n",
      "Epoch [192/20000], Train Loss: 0.0066, Val Loss: 0.0064\n",
      "Epoch [193/20000], Train Loss: 0.0065, Val Loss: 0.0065\n",
      "Epoch [194/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [195/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [196/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [197/20000], Train Loss: 0.0065, Val Loss: 0.0067\n",
      "Epoch [198/20000], Train Loss: 0.0065, Val Loss: 0.0066\n",
      "Epoch [199/20000], Train Loss: 0.0066, Val Loss: 0.0065\n",
      "Epoch [200/20000], Train Loss: 0.0065, Val Loss: 0.0066\n",
      "Epoch [201/20000], Train Loss: 0.0066, Val Loss: 0.0064\n",
      "Epoch [202/20000], Train Loss: 0.0066, Val Loss: 0.0062\n",
      "Epoch [203/20000], Train Loss: 0.0065, Val Loss: 0.0066\n",
      "Epoch [204/20000], Train Loss: 0.0066, Val Loss: 0.0066\n",
      "Epoch [205/20000], Train Loss: 0.0065, Val Loss: 0.0066\n",
      "Epoch [206/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [207/20000], Train Loss: 0.0066, Val Loss: 0.0064\n",
      "Epoch [208/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [209/20000], Train Loss: 0.0065, Val Loss: 0.0065\n",
      "Epoch [210/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [211/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [212/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [213/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [214/20000], Train Loss: 0.0065, Val Loss: 0.0066\n",
      "Epoch [215/20000], Train Loss: 0.0066, Val Loss: 0.0070\n",
      "Epoch [216/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [217/20000], Train Loss: 0.0066, Val Loss: 0.0066\n",
      "Epoch [218/20000], Train Loss: 0.0066, Val Loss: 0.0062\n",
      "Epoch [219/20000], Train Loss: 0.0065, Val Loss: 0.0069\n",
      "Epoch [220/20000], Train Loss: 0.0065, Val Loss: 0.0065\n",
      "Epoch [221/20000], Train Loss: 0.0065, Val Loss: 0.0071\n",
      "Epoch [222/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [223/20000], Train Loss: 0.0065, Val Loss: 0.0065\n",
      "Epoch [224/20000], Train Loss: 0.0065, Val Loss: 0.0067\n",
      "Epoch [225/20000], Train Loss: 0.0066, Val Loss: 0.0067\n",
      "Epoch [226/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [227/20000], Train Loss: 0.0066, Val Loss: 0.0063\n",
      "Epoch [228/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [229/20000], Train Loss: 0.0065, Val Loss: 0.0078\n",
      "Epoch [230/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [231/20000], Train Loss: 0.0065, Val Loss: 0.0067\n",
      "Epoch [232/20000], Train Loss: 0.0065, Val Loss: 0.0062\n",
      "Epoch [233/20000], Train Loss: 0.0065, Val Loss: 0.0062\n",
      "Epoch [234/20000], Train Loss: 0.0065, Val Loss: 0.0065\n",
      "Epoch [235/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [236/20000], Train Loss: 0.0066, Val Loss: 0.0065\n",
      "Epoch [237/20000], Train Loss: 0.0065, Val Loss: 0.0066\n",
      "Epoch [238/20000], Train Loss: 0.0065, Val Loss: 0.0061\n",
      "Epoch [239/20000], Train Loss: 0.0065, Val Loss: 0.0072\n",
      "Epoch [240/20000], Train Loss: 0.0065, Val Loss: 0.0060\n",
      "Epoch [241/20000], Train Loss: 0.0065, Val Loss: 0.0062\n",
      "Epoch [242/20000], Train Loss: 0.0066, Val Loss: 0.0062\n",
      "Epoch [243/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [244/20000], Train Loss: 0.0066, Val Loss: 0.0062\n",
      "Epoch [245/20000], Train Loss: 0.0065, Val Loss: 0.0072\n",
      "Epoch [246/20000], Train Loss: 0.0066, Val Loss: 0.0071\n",
      "Epoch [247/20000], Train Loss: 0.0065, Val Loss: 0.0067\n",
      "Epoch [248/20000], Train Loss: 0.0065, Val Loss: 0.0067\n",
      "Epoch [249/20000], Train Loss: 0.0065, Val Loss: 0.0066\n",
      "Epoch [250/20000], Train Loss: 0.0065, Val Loss: 0.0068\n",
      "Epoch [251/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [252/20000], Train Loss: 0.0065, Val Loss: 0.0068\n",
      "Epoch [253/20000], Train Loss: 0.0065, Val Loss: 0.0062\n",
      "Epoch [254/20000], Train Loss: 0.0066, Val Loss: 0.0062\n",
      "Epoch [255/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [256/20000], Train Loss: 0.0065, Val Loss: 0.0069\n",
      "Epoch [257/20000], Train Loss: 0.0065, Val Loss: 0.0065\n",
      "Epoch [258/20000], Train Loss: 0.0065, Val Loss: 0.0066\n",
      "Epoch [259/20000], Train Loss: 0.0065, Val Loss: 0.0062\n",
      "Epoch [260/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [261/20000], Train Loss: 0.0066, Val Loss: 0.0064\n",
      "Epoch [262/20000], Train Loss: 0.0065, Val Loss: 0.0072\n",
      "Epoch [263/20000], Train Loss: 0.0065, Val Loss: 0.0067\n",
      "Epoch [264/20000], Train Loss: 0.0065, Val Loss: 0.0067\n",
      "Epoch [265/20000], Train Loss: 0.0065, Val Loss: 0.0062\n",
      "Epoch [266/20000], Train Loss: 0.0065, Val Loss: 0.0061\n",
      "Epoch [267/20000], Train Loss: 0.0065, Val Loss: 0.0071\n",
      "Epoch [268/20000], Train Loss: 0.0065, Val Loss: 0.0065\n",
      "Epoch [269/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [270/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [271/20000], Train Loss: 0.0066, Val Loss: 0.0066\n",
      "Epoch [272/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [273/20000], Train Loss: 0.0066, Val Loss: 0.0071\n",
      "Epoch [274/20000], Train Loss: 0.0065, Val Loss: 0.0069\n",
      "Epoch [275/20000], Train Loss: 0.0066, Val Loss: 0.0067\n",
      "Epoch [276/20000], Train Loss: 0.0065, Val Loss: 0.0061\n",
      "Epoch [277/20000], Train Loss: 0.0066, Val Loss: 0.0062\n",
      "Epoch [278/20000], Train Loss: 0.0066, Val Loss: 0.0064\n",
      "Epoch [279/20000], Train Loss: 0.0066, Val Loss: 0.0064\n",
      "Epoch [280/20000], Train Loss: 0.0066, Val Loss: 0.0064\n",
      "Epoch [281/20000], Train Loss: 0.0065, Val Loss: 0.0065\n",
      "Epoch [282/20000], Train Loss: 0.0065, Val Loss: 0.0068\n",
      "Epoch [283/20000], Train Loss: 0.0065, Val Loss: 0.0065\n",
      "Epoch [284/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [285/20000], Train Loss: 0.0065, Val Loss: 0.0065\n",
      "Epoch [286/20000], Train Loss: 0.0066, Val Loss: 0.0065\n",
      "Epoch [287/20000], Train Loss: 0.0065, Val Loss: 0.0069\n",
      "Epoch [288/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [289/20000], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [290/20000], Train Loss: 0.0065, Val Loss: 0.0066\n",
      "Epoch [291/20000], Train Loss: 0.0065, Val Loss: 0.0065\n",
      "Epoch [292/20000], Train Loss: 0.0065, Val Loss: 0.0068\n",
      "Epoch [293/20000], Train Loss: 0.0065, Val Loss: 0.0065\n",
      "Epoch [294/20000], Train Loss: 0.0065, Val Loss: 0.0069\n",
      "Epoch [295/20000], Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Epoch [296/20000], Train Loss: 0.0066, Val Loss: 0.0067\n",
      "Epoch [297/20000], Train Loss: 0.0065, Val Loss: 0.0065\n",
      "Epoch [298/20000], Train Loss: 0.0065, Val Loss: 0.0067\n",
      "Epoch [299/20000], Train Loss: 0.0065, Val Loss: 0.0061\n",
      "Epoch [300/20000], Train Loss: 0.0065, Val Loss: 0.0071\n",
      "Epoch [301/20000], Train Loss: 0.0065, Val Loss: 0.0065\n",
      "Epoch [302/20000], Train Loss: 0.0065, Val Loss: 0.0065\n",
      "Epoch [303/20000], Train Loss: 0.0065, Val Loss: 0.0065\n",
      "Epoch [304/20000], Train Loss: 0.0066, Val Loss: 0.0065\n",
      "Epoch [305/20000], Train Loss: 0.0065, Val Loss: 0.0067\n"
     ]
    }
   ],
   "source": [
    "# Define the number of epochs and batch size\n",
    "num_epochs = 20000\n",
    "batch_size = 32\n",
    "\n",
    "# Define the data loaders for the training and validation sets\n",
    "train_dataset = DNAReadsDataset(dataset1_reads[:split_size], dataset2_reads[:split_size], train_similarities)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = DNAReadsDataset(dataset1_reads[split_size:], dataset2_reads[split_size:], val_similarities)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Train the siamese autoencoder model\n",
    "for epoch in range(num_epochs):\n",
    "    # Train the model on the training set\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (input1, input2, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input1, input2)\n",
    "        loss = contrastive_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input1, input2, target) in enumerate(val_loader):\n",
    "            output = model(input1, input2)\n",
    "            loss = contrastive_loss(output, target)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    # Print the loss for this epoch\n",
    "    print('Epoch [{}/{}], Train Loss: {:.4f}, Val Loss: {:.4f}'.format(epoch+1, num_epochs, train_loss, val_loss))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lyn2vec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
